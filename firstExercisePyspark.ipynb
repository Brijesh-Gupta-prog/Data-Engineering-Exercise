{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ced10682-95ef-4530-b0dd-9d9b20fcc548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "WXiPRqJKqn3w"
   },
   "source": [
    "# Module 01 - Introduction & SparkSession - Exercises\n",
    "\n",
    "## Instructions\n",
    "\n",
    "This notebook contains exercises based on the concepts learned in Module 01.\n",
    "\n",
    "- Complete each exercise in the provided code cells\n",
    "- Run the data setup cells first to generate/create necessary data\n",
    "- Test your solutions by running the verification cells (if provided)\n",
    "- Refer back to the main module notebook if you need help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dccec932-5927-4631-8640-ad525a672931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "H6EyeBcXqn3x"
   },
   "source": [
    "## Data Setup\n",
    "\n",
    "Run the cells below to set up the data needed for the exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57b9f06c-516e-4732-a07f-c4ab0d85de5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VHkTFp4bqn3x",
    "outputId": "9269716b-f9d8-46d2-f9b2-3f8394f60eba"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "import os\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Module 01 Exercises\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set data directory\n",
    "data_dir = \"../data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "print(\"SparkSession created successfully!\")\n",
    "print(f\"Data directory: {os.path.abspath(data_dir)}\")\n",
    "\n",
    "# Create sample CSV file for exercises\n",
    "sample_csv = \"\"\"Name,Age,Department,Salary\n",
    "Alice,25,Sales,50000\n",
    "Bob,30,IT,60000\n",
    "Charlie,35,Sales,70000\n",
    "Diana,28,IT,55000\n",
    "Eve,32,HR,65000\"\"\"\n",
    "\n",
    "with open(f\"{data_dir}/employees.csv\", \"w\") as f:\n",
    "    f.write(sample_csv)\n",
    "print(\"Sample CSV file created: employees.csv\")\n",
    "\n",
    "# Create sample JSON file for exercises\n",
    "sample_json = \"\"\"{\"name\": \"John\", \"age\": 28, \"city\": \"NYC\"}\n",
    "{\"name\": \"Jane\", \"age\": 32, \"city\": \"LA\"}\n",
    "{\"name\": \"Mike\", \"age\": 25, \"city\": \"Chicago\"}\n",
    "{\"name\": \"Sarah\", \"age\": 30, \"city\": \"Houston\"}\"\"\"\n",
    "\n",
    "with open(f\"{data_dir}/people.json\", \"w\") as f:\n",
    "    f.write(sample_json)\n",
    "print(\"Sample JSON file created: people.json\")\n",
    "\n",
    "# Create sample Parquet file for exercises\n",
    "product_data = [\n",
    "    (1, \"Product A\", 100, \"Electronics\"),\n",
    "    (2, \"Product B\", 200, \"Clothing\"),\n",
    "    (3, \"Product C\", 150, \"Electronics\"),\n",
    "    (4, \"Product D\", 300, \"Home\"),\n",
    "    (5, \"Product E\", 250, \"Clothing\")\n",
    "]\n",
    "\n",
    "product_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"price\", IntegerType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_products_temp = spark.createDataFrame(product_data, product_schema)\n",
    "df_products_temp.write.mode(\"overwrite\").parquet(f\"{data_dir}/products.parquet\")\n",
    "print(\"Sample Parquet file created: products.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2b12ce8-b072-4d1e-9852-2c0ea3003258",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "R0iqxxRfqn3y"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "Complete the following exercises based on the concepts from Module 01.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76fd0874-bd69-461f-b8f4-d0c1ebc2dc02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "xLWXPCHcqn3y"
   },
   "source": [
    "### Exercise 1: Create a SparkSession\n",
    "\n",
    "Create a SparkSession with app name 'MyFirstSparkApp' and master set to 'local[*]'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "417652b6-d8ed-4518-97be-5bdfc4b8facc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9uvlWZPvqn3y"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "spark = SparkSession.builder \\\n",
    "          .appName('MyFirstSparkApp') \\\n",
    "          .master('local[*]') \\\n",
    "          .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efbd6630-e8c9-464d-bc6c-dbf843e8a0ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "svPlRaKzqn3y"
   },
   "source": [
    "### Exercise 2: Create a DataFrame\n",
    "\n",
    "Create a DataFrame with the following data:\n",
    "- Names: ['John', 'Jane', 'Mike', 'Sarah']\n",
    "- Ages: [28, 32, 25, 30]\n",
    "- Cities: ['NYC', 'LA', 'Chicago', 'Houston']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "190774cf-d499-49dd-92c2-5cbfb00ad017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3JH0F5GZqn3y",
    "outputId": "fa9ba97b-561f-49e1-c2e0-f149ad13dd82"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "data = [\n",
    "    ('John', 28, 'NYC'),\n",
    "    ('Jane', 32, 'LA'),\n",
    "    ('Mike', 25, 'Chicago'),\n",
    "    ('Sarah', 30, 'Houston')\n",
    "]\n",
    "df_2 = spark.createDataFrame(data,['Name','Age','City'])\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da3e1f31-6a16-4c6e-9dbb-7a4fab31bfcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "qMRx4Rf9qn3y"
   },
   "source": [
    "### Exercise 3: Display DataFrame\n",
    "\n",
    "Display the first 3 rows of the DataFrame you created in Exercise 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fc4bdef-ebe9-4b38-a157-2a4ef9747bb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRhDkXnAqn3z",
    "outputId": "0a734e91-ab03-4e86-97af-63eef3673183"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df.show(3)\n",
    "# Hint: Use the show() method with the numRows parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b92c99f-fe58-4f3c-856f-3de3f814db25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "2mZ1mX9aqn3z"
   },
   "source": [
    "### Exercise 4: Create DataFrame with Explicit Schema\n",
    "\n",
    "Create a DataFrame with the following data and explicit schema:\n",
    "- Data: [(\"Product A\", 100.50, 10), (\"Product B\", 200.75, 5), (\"Product C\", 150.25, 8)]\n",
    "- Schema:\n",
    "  - Name: StringType\n",
    "  - Price: DoubleType\n",
    "  - Quantity: IntegerType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "858284b3-5709-4436-ae6d-3f0ae5361527",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KGW26eOsqn3z",
    "outputId": "9bd55c9e-bebb-41a1-820a-1dd0a33c7d34"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "product = [(\"Product A\", 100.50, 10), (\"Product B\", 200.75, 5), (\"Product C\", 150.25, 8)]\n",
    "product_schema = StructType([\n",
    "    StructField('product_id',StringType(),True),\n",
    "    StructField('product_price',DoubleType(),True),\n",
    "    StructField('No_Of_Product',IntegerType(),True)\n",
    "])\n",
    "df_4 = spark.createDataFrame(product,product_schema)\n",
    "df_4.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da8e78d4-df12-4524-b6eb-e3469b5a9ebe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "lCMI0O45qn3z"
   },
   "source": [
    "### Exercise 5: Create DataFrame using spark.range\n",
    "\n",
    "Create a DataFrame using `spark.range()` that contains numbers from 1 to 20 (inclusive of 1, exclusive of 20). Then display the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9956cb22-7a42-4fd2-87d4-20c6478f0003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9rRRen4Qqn3z",
    "outputId": "7cc294eb-d1c2-4932-d6cc-371a0c17a12a"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df = spark.range(1,20)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98a09ccf-7dcf-4efc-b1a9-9198370056ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "H0z_hETyqn3z"
   },
   "source": [
    "### Exercise 6: Create DataFrame from CSV File\n",
    "\n",
    "Read the CSV file `employees.csv` from the data directory using `spark.read`. Use schema inference (header=True, inferSchema=True). Display the DataFrame and print its schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11afcb7e-2a4b-4ae9-aa7d-ecd66b097580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1N9MNh-Pqn3z",
    "outputId": "82b80ee3-e1e4-4aea-ad10-e0d8c28428e0"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df_6 = spark.read \\\n",
    "      .format('csv') \\\n",
    "      .option('header','true') \\\n",
    "      .option('inferSchema','true') \\\n",
    "      .load(f'{data_dir}/employees.csv')\n",
    "df_6.show()\n",
    "df_6.printSchema()\n",
    "# Hint: Use spark.read.format(\"csv\") with appropriate options\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e885ee80-e9ba-47e5-8d81-c6fd939d0cbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "U3FFwgU5qn3z"
   },
   "source": [
    "### Exercise 7: Create DataFrame from JSON File\n",
    "\n",
    "Read the JSON file `people.json` from the data directory using `spark.read`. Display the DataFrame and print its schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccb96bce-0b8d-44c1-9027-573043a5c7b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68n6z-Wkqn3z",
    "outputId": "8d91ae2a-d02b-4a50-caf0-4c45d3b942ca"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df = spark.read \\\n",
    "     .format(\"json\") \\\n",
    "     .option(\"header\",\"true\") \\\n",
    "     .option(\"inferSchema\",\"true\") \\\n",
    "     .load(f\"{data_dir}/people.json\")\n",
    "df.show()\n",
    "df.printSchema()\n",
    "# Hint: Use spark.read.format(\"json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47e8bc90-1ae0-46a3-9b21-09fe712fb227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "wl3Lv_LLqn3z"
   },
   "source": [
    "### Exercise 8: Create DataFrame using spark.sql\n",
    "\n",
    "First, create a temporary view from the DataFrame you created in Exercise 2 (the one with John, Jane, Mike, Sarah). Then use `spark.sql()` to create a new DataFrame that selects only the Name and Age columns where Age is greater than 26. Display the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "877c9253-b9f4-4f9c-876a-b922c58f45a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xa6yqxZUqn3z",
    "outputId": "04d8c5a2-5c47-424d-e185-2cd095891d1a"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df_2.createOrReplaceTempView(\"random\")\n",
    "df = spark.sql('SELECT Name,Age from random where age>26')\n",
    "df.show()\n",
    "df.printSchema()\n",
    "# Hint: Use createOrReplaceTempView() to register the DataFrame, then use spark.sql()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "828f6f8f-877e-4b78-9dbd-29730a6207c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "lnJkdAMsqn3z"
   },
   "source": [
    "### Exercise 9: Create DataFrame from CSV with Explicit Schema\n",
    "\n",
    "Read the `employees.csv` file again, but this time use an explicit schema instead of schema inference. Define a schema with:\n",
    "- Name: StringType\n",
    "- Age: IntegerType\n",
    "- Department: StringType\n",
    "- Salary: IntegerType\n",
    "\n",
    "Display the DataFrame and print its schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b3d5f5e-a879-42eb-9cc0-c16211bb854c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TC_9OVJhqn3z",
    "outputId": "b7568c9c-a083-49ef-ce7f-eb9c8be7c316"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "schema = StructType([\n",
    "    StructField(\"Name\",StringType(),True),\n",
    "    StructField(\"Age\",IntegerType(),True),\n",
    "    StructField(\"Department\",StringType(),True),\n",
    "    StructField(\"Salary\",IntegerType(),True)\n",
    "])\n",
    "df = spark.read.schema(schema).csv(f\"{data_dir}/employees.csv\")\n",
    "df.show()\n",
    "df.printSchema()\n",
    "# Hint: Define a StructType schema first, then use .schema() in the read operation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0859d03-ff69-4d33-bf21-3575c80e10f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Hg4L5ZUwqn30"
   },
   "source": [
    "### Exercise 10: Access DataFrame Schema Information\n",
    "\n",
    "Using the DataFrame from Exercise 6 (CSV file), print the schema and then access the data type of the \"Age\" column from the schema. Display both the full schema and the specific column's data type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "282aecf9-fa02-43ae-99ab-27a7e8ab5ca6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-cQUhieqn30",
    "outputId": "c4130b47-5b75-445b-c399-54c521e68d3f"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df_6.printSchema()\n",
    "data_type = df_6.schema['Age'].dataType\n",
    "print(data_type)\n",
    "# Hint: Use printSchema() and then access schema['ColumnName'].dataType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e373ea7-de50-4430-9c70-fc79dc5faebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "sOmdHeZUqn30"
   },
   "source": [
    "### Exercise 11: Get DataFrame Basic Information\n",
    "\n",
    "Using the DataFrame from Exercise 2, get and print:\n",
    "1. The number of rows\n",
    "2. The list of column names\n",
    "3. The number of columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e9e88d0-1074-473a-acc7-b568ebcefbde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hg5JBuW-qn30",
    "outputId": "de6c94f3-5a81-4a43-c20f-ba425ca982e7"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "print(f\"Number of Rows {df_2.count()}\")\n",
    "print(f\"The list of column names {df_2.columns}\")\n",
    "print(f\"The number of columns {len(df_2.columns)}\")\n",
    "# Hint: Use count(), columns, and len()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dbde171-a76f-4da1-ba5d-2b7ec9a14ea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "UHD7xN6Aqn30"
   },
   "source": [
    "### Exercise 12: Create DataFrame using spark.table\n",
    "\n",
    "First, create a temporary view called \"employees_view\" from the DataFrame you created in Exercise 6 (CSV DataFrame). Then use `spark.table()` to read from this view and create a new DataFrame. Display the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7a7be5a-fea2-4f83-b5c2-ea4c60c36d29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QiBU-973qn30",
    "outputId": "8d3c512e-c295-4571-a395-ef79f6d01d89"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df_6.createOrReplaceTempView(\"employees_view\")\n",
    "df_11 = spark.table(\"employees_view\")\n",
    "df_11.show()\n",
    "# Hint: Use createOrReplaceTempView() first, then spark.table()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfe12967-8a91-4508-8d11-43c3284af1d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "s6lt2qtDqn30"
   },
   "source": [
    "### Exercise 13: Create DataFrame using spark.sql with VALUES\n",
    "\n",
    "Use `spark.sql()` with the VALUES clause to create a DataFrame with the following data:\n",
    "- ('Apple', 1.50, 10)\n",
    "- ('Banana', 0.75, 20)\n",
    "- ('Orange', 2.00, 15)\n",
    "\n",
    "Name the columns: Product, Price, Stock. Display the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e2ddc2b-bcde-4842-aad4-23a1d24a1339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O-OgGF37qn30",
    "outputId": "0fe86b7a-d998-46db-9739-9a6175717ecf"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df_13 = spark.sql(\"SELECT * FROM VALUES ('Apple', 1.50, 10),('Banana', 0.75, 20),('Orange', 2.00, 15) AS t(Product,Price,Stock)\")\n",
    "df_13.show()\n",
    "# Hint: Use spark.sql(\"SELECT * FROM VALUES (...) AS t(columns)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24f985ce-7b51-4ec9-a0d3-ce3d3ce24ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "1a3Gtu21qn30"
   },
   "source": [
    "### Exercise 14: Create DataFrame from Parquet File\n",
    "\n",
    "Read the Parquet file `products.parquet` from the data directory using `spark.read`. Display the DataFrame, print its schema, and show the number of rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eafb796f-485f-49bc-9d28-fe1c557f15d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_n49jcnuqn30",
    "outputId": "ab142d64-319c-4d9f-b644-ba8502cafd48"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df_14 = spark.read \\\n",
    "        .format(\"parquet\") \\\n",
    "        .load(f\"{data_dir}/products.parquet\")\n",
    "\n",
    "df_14.show()\n",
    "df_14.printSchema()\n",
    "print(f\"Number of rows {df_14.count()}\")\n",
    "# Hint: Use spark.read.format(\"parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12d30ca2-70aa-4ae6-9980-e6caec5ee97d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "BbX9IRjeqn30"
   },
   "source": [
    "### Exercise 15: Create DataFrame using .toDF() Method\n",
    "\n",
    "Create a DataFrame from a list of tuples without specifying column names in `createDataFrame()`. Then use the `.toDF()` method to assign column names: \"Student\", \"Score\", \"Grade\". Display the DataFrame.\n",
    "\n",
    "Data: [(\"Alice\", 95, \"A\"), (\"Bob\", 87, \"B\"), (\"Charlie\", 92, \"A\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a88866e-e6df-448c-bc43-62c405a8d661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3opUut9qn30",
    "outputId": "183639f2-6c98-47a7-d176-da2eab07bbbc"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "Data = [(\"Alice\", 95, \"A\"), (\"Bob\", 87, \"B\"), (\"Charlie\", 92, \"A\")]\n",
    "df_15 = spark.createDataFrame(Data).toDF(\"Name\",\"Marks\",\"Grade\")\n",
    "df_15.show()\n",
    "# Hint: spark.createDataFrame(data).toDF(\"col1\", \"col2\", \"col3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33b9b0b2-739c-4685-8303-67292ea61d8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "q_cTJj70qn30"
   },
   "source": [
    "### Exercise 16: Get DataFrame Column Data Types\n",
    "\n",
    "Using the DataFrame from Exercise 4 (products with explicit schema), get and print the data types of all columns. Use the `dtypes` property of the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62137958-84c6-4c63-b43b-d5a5235fce83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvXk8leQqn30",
    "outputId": "0cca602e-27af-4bf8-f554-3ad3d4c537fa"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "for column_name,data_type in df_4.dtypes:\n",
    "  print(f\"Column :{column_name} And Data Types :{data_type}\")\n",
    "# Hint: Use the .dtypes property\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f46f794-a686-456f-a116-3b5b4cc308df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "T7AmIevaqn30"
   },
   "source": [
    "### Exercise 17: Create DataFrame with spark.range and Custom Step\n",
    "\n",
    "Create a DataFrame using `spark.range()` that contains even numbers from 0 to 20 (exclusive of 20). Use the step parameter. Display the DataFrame and verify it contains only even numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3f4e12a-50f7-4aa8-8db2-d29b9cfef46f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nkMVbHigqn31",
    "outputId": "7d09ba10-4dd7-41f1-de0f-36bc64b50f34"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df_17 = spark.range(0,20,2)\n",
    "df_17.show()\n",
    "# Hint: spark.range(start, end, step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acef946d-fa61-4e80-a71a-6cbe006ac819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "UrIewyNEqn31"
   },
   "source": [
    "### Exercise 18: Check if DataFrame is Empty\n",
    "\n",
    "Create an empty DataFrame using `spark.range(0, 0)` and check if it's empty using the `isEmpty()` method. Then create a non-empty DataFrame and check again. Print both results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ad937a2-a14a-4940-96e3-5cdf3b288bc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5euaEObqn31",
    "outputId": "267f32eb-60c4-4865-e0cb-9cd68d118dca"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df_first = spark.range(0,0)\n",
    "print(df_first.isEmpty())\n",
    "df_second = spark.range(3,16)\n",
    "print(df_second.isEmpty())\n",
    "# Hint: Use isEmpty() method on DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14cf71a5-a060-4360-9974-769f321877c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8JR0y30Eqn31"
   },
   "source": [
    "### Exercise 19: Access Schema Fields\n",
    "\n",
    "Using the DataFrame from Exercise 4, access the schema object and print:\n",
    "1. The number of fields in the schema\n",
    "2. The name and data type of the first field\n",
    "3. The name and data type of the \"Price\" field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57ab8386-40f4-4b3f-8356-5d9051f1e119",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v6NrDxE3qn32",
    "outputId": "72a80906-9e42-465f-de08-ed3d3565760c"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "print(f\"The number of fields in the schema is {len(df_4.schema.fields)}\")\n",
    "print(f\"The name and data type of the first field is {df_4.schema.fields[0]}\")\n",
    "#print(f\"The name and data type of the price field is {df_4.schema.fields['product_price']}\")\n",
    "field = next((field for field in df_4.schema.fields if field.name == 'product_price'),None)\n",
    "if field:\n",
    "  print(f\"field name : {field.name} and field datatype is {field.dataType}\")\n",
    "# Hint: Use df.schema.fields and access field properties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb2b7c60-30e3-4cb6-8704-9891b09867cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "H2S55DItqn32"
   },
   "source": [
    "### Exercise 20: Create DataFrame - Multiple Methods Comparison\n",
    "\n",
    "Create the same DataFrame using three different methods:\n",
    "1. Using `spark.createDataFrame()` with column names\n",
    "2. Using `spark.createDataFrame()` with `.toDF()`\n",
    "3. Using `spark.sql()` with VALUES\n",
    "\n",
    "Data: [(\"X\", 10), (\"Y\", 20), (\"Z\", 30)]\n",
    "Columns: \"Letter\", \"Number\"\n",
    "\n",
    "Display all three DataFrames to verify they're the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9af29129-690a-484c-8ebf-781bdc7e46b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QqKyCAMfqn32",
    "outputId": "94ec5409-def2-40bf-971d-bd84b7780268"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "Data = [(\"X\", 10), (\"Y\", 20), (\"Z\", 30)]\n",
    "first_method = spark.createDataFrame(Data,[\"Letter\", \"Number\"])\n",
    "second_method = spark.createDataFrame(Data).toDF(\"Letter\", \"Number\")\n",
    "third_method = spark.sql(\"SELECT * FROM VALUES ('X', 10), ('Y', 20), ('Z', 30) AS t(Letter, Number)\")\n",
    "first_method.show()\n",
    "second_method.show()\n",
    "third_method.show()\n",
    "# Create three DataFrames using different methods and display them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f33192eb-2827-4feb-9a70-9dc9ebb0e434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "pz33U13Pqn32"
   },
   "source": [
    "## Summary\n",
    "\n",
    "Review your solutions and compare them with the solutions notebook if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bbf7259-37de-4bf4-8471-514d6345ed1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8a6OvgDpqn32"
   },
   "source": [
    "<< end of notebook >>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "firstExercisePyspark",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
