{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61236d61-8b79-4c91-8f4b-24b6feda7e5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spark SQL Practice - Revenue Analysis\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook contains a practice question on computing monthly revenue by region using Spark SQL. This exercise will help you master:\n",
    "\n",
    "- Date filtering and window functions\n",
    "- Handling latest records per group\n",
    "- Joins across multiple tables\n",
    "- Monthly aggregation with date functions\n",
    "- Complex business logic implementation\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. **In Databricks**: SparkSession is automatically available as `spark`\n",
    "2. **For local testing**: Uncomment the SparkSession creation code in the setup cell\n",
    "3. Run the data setup cells first to create sample data\n",
    "4. Complete the exercise in the provided code cell\n",
    "5. Test your solution and verify the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5074eda-5021-4fea-975c-6c8b2abaeae3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Setup\n",
    "\n",
    "Run the cells below to set up all the sample data needed for the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c591e2d-94da-4be9-aa52-a734d61cdba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# In Databricks, SparkSession is already available\n",
    "# For local testing, uncomment the following:\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Spark SQL Practice\") \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import col, to_timestamp, current_timestamp, expr, date_sub, date_format, sum as spark_sum, max as spark_max, row_number, window\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"Setup complete! SparkSession ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bdc0842-9b56-4ed7-8c1a-3fd8735c4026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create customers table\n",
    "# Schema: customer_id, region\n",
    "\n",
    "customers_data = [\n",
    "    (1, \"North\"),\n",
    "    (2, \"South\"),\n",
    "    (3, \"East\"),\n",
    "    (4, \"West\"),\n",
    "    (5, \"North\"),\n",
    "    (6, \"South\"),\n",
    "    (7, \"East\"),\n",
    "    (8, \"West\"),\n",
    "    (9, \"North\"),\n",
    "    (10, \"South\")\n",
    "]\n",
    "\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_customers = spark.createDataFrame(customers_data, customers_schema)\n",
    "df_customers.createOrReplaceTempView(\"customers\")\n",
    "\n",
    "print(\"Customers table created:\")\n",
    "df_customers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6e2dfc2-d2d2-4096-b5a2-42af0a42e27a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create orders table\n",
    "# Schema: order_id, customer_id, order_ts, amount\n",
    "# We'll create orders spanning the last 120 days to have data beyond the 90-day window\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Get current timestamp\n",
    "current_ts = datetime.now()\n",
    "\n",
    "# Generate orders over the last 120 days\n",
    "orders_data = []\n",
    "order_id = 1\n",
    "\n",
    "# Create orders for each customer across different dates\n",
    "for customer_id in range(1, 11):\n",
    "    # Create 2-4 orders per customer at different dates\n",
    "    num_orders = random.randint(2, 4)\n",
    "    for _ in range(num_orders):\n",
    "        # Random date within last 120 days\n",
    "        days_ago = random.randint(0, 120)\n",
    "        order_date = current_ts - timedelta(days=days_ago)\n",
    "        order_ts = order_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        amount = round(random.uniform(100, 2000), 2)\n",
    "        orders_data.append((order_id, customer_id, order_ts, amount))\n",
    "        order_id += 1\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_ts\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df_orders = spark.createDataFrame(orders_data, orders_schema)\n",
    "# Convert order_ts to timestamp type\n",
    "df_orders = df_orders.withColumn(\"order_ts\", to_timestamp(col(\"order_ts\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df_orders.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "print(\"Orders table created:\")\n",
    "df_orders.orderBy(\"order_ts\").show(50, truncate=False)\n",
    "print(f\"\\nTotal orders: {df_orders.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07b633e8-4fb7-4e90-8413-e58343044636",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create payments table\n",
    "# Schema: payment_id, order_id, amount, paid_ts\n",
    "# Note: An order can have multiple payments (partial payments, refunds, etc.)\n",
    "# We need to identify the LATEST payment per order\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Get current timestamp\n",
    "current_ts = datetime.now()\n",
    "\n",
    "payments_data = []\n",
    "payment_id = 1\n",
    "\n",
    "# For each order, create 1-3 payments at different times\n",
    "for order_row in orders_data:\n",
    "    order_id = order_row[0]\n",
    "    order_date_str = order_row[2]\n",
    "    order_date = datetime.strptime(order_date_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    order_amount = order_row[3]\n",
    "    \n",
    "    # Create 1-3 payments per order\n",
    "    num_payments = random.randint(1, 3)\n",
    "    remaining_amount = order_amount\n",
    "    \n",
    "    for i in range(num_payments):\n",
    "        # Payment date is after order date, within 30 days\n",
    "        days_after_order = random.randint(0, 30)\n",
    "        payment_date = order_date + timedelta(days=days_after_order, hours=random.randint(0, 23))\n",
    "        paid_ts = payment_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # Last payment gets remaining amount, others are partial\n",
    "        if i == num_payments - 1:\n",
    "            payment_amount = round(remaining_amount, 2)\n",
    "        else:\n",
    "            payment_amount = round(random.uniform(0.1, remaining_amount * 0.8), 2)\n",
    "            remaining_amount -= payment_amount\n",
    "        \n",
    "        payments_data.append((payment_id, order_id, payment_amount, paid_ts))\n",
    "        payment_id += 1\n",
    "\n",
    "payments_schema = StructType([\n",
    "    StructField(\"payment_id\", IntegerType(), True),\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"paid_ts\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_payments = spark.createDataFrame(payments_data, payments_schema)\n",
    "# Convert paid_ts to timestamp type\n",
    "df_payments = df_payments.withColumn(\"paid_ts\", to_timestamp(col(\"paid_ts\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df_payments.createOrReplaceTempView(\"payments\")\n",
    "\n",
    "print(\"Payments table created:\")\n",
    "df_payments.orderBy(\"order_id\", \"paid_ts\").show(50, truncate=False)\n",
    "print(f\"\\nTotal payments: {df_payments.count()}\")\n",
    "\n",
    "# Show example: multiple payments for same order\n",
    "print(\"\\nExample: Multiple payments for order_id = 1:\")\n",
    "df_payments.filter(col(\"order_id\") == 1).orderBy(\"paid_ts\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "544330c2-97a1-4a94-a4d7-d1452b476bac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Practice Question\n",
    "\n",
    "### Task 1: Monthly Revenue by Region (Last 90 Days)\n",
    "\n",
    "**Requirement**: For the last 90 days, compute monthly revenue by region based on the **latest payment per order**.\n",
    "\n",
    "**Key Points to Consider:**\n",
    "1. Filter to last 90 days based on payment date (`paid_ts`)\n",
    "2. For each order, use only the **latest payment** (most recent `paid_ts`)\n",
    "3. Join with customers to get the region\n",
    "4. Group by month and region\n",
    "5. Sum the payment amounts\n",
    "\n",
    "**Tables:**\n",
    "- `customers(customer_id, region)`\n",
    "- `orders(order_id, customer_id, order_ts, amount)`\n",
    "- `payments(payment_id, order_id, amount, paid_ts)`\n",
    "\n",
    "**Expected Output Columns:**\n",
    "- `month` (e.g., \"2024-01\", \"2024-02\")\n",
    "- `region`\n",
    "- `revenue` (sum of latest payment amounts)\n",
    "\n",
    "**Hints:**\n",
    "- Use window functions to identify the latest payment per order\n",
    "- Consider using CTEs (Common Table Expressions) to break down the problem\n",
    "- Remember to filter by date before applying window functions for better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f92598b9-82a6-426f-99bb-a42df0776427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4e97569-a014-4812-8002-30802fff05f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "WITH LATEST AS (\n",
    " SELECT order_id, paid_ts, amount\n",
    "FROM (\n",
    "    SELECT *,\n",
    "           ROW_NUMBER() OVER (PARTITION BY order_id ORDER BY paid_ts DESC) AS rn\n",
    "    FROM payments\n",
    ") t\n",
    "WHERE rn = 1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b968b496-534f-4cfc-a7cc-6626fb186210",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "WITH minimum_date AS (\n",
    "  SELECT MIN(\n",
    "    DATEDIFF(current_date, order_ts) \n",
    ") as min_days\n",
    "FROM orders\n",
    ")\n",
    "select * from orders\n",
    "WHERE DATEDIFF(current_date,order_ts) = (SELECT min_days FROM minimum_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08379acb-29bb-4422-a0f3-039c110140eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "WITH latest_payments AS (\n",
    "    -- Step 1: Get the latest payment for each order\n",
    "    SELECT\n",
    "        order_id,\n",
    "        amount,\n",
    "        paid_ts,\n",
    "        ROW_NUMBER() OVER (PARTITION BY order_id ORDER BY paid_ts DESC) as rn\n",
    "    FROM payments\n",
    "    WHERE paid_ts >= current_timestamp() - INTERVAL 90 DAYS\n",
    "),\n",
    "latest_payment_per_order AS (\n",
    "    -- Step 2: Filter to only the latest payment (rn = 1)\n",
    "    SELECT\n",
    "        order_id,\n",
    "        amount as payment_amount,\n",
    "        paid_ts\n",
    "    FROM latest_payments\n",
    "    WHERE rn = 1\n",
    "),\n",
    "orders_with_payments AS (\n",
    "    -- Step 3: Join orders with latest payments and customers\n",
    "    SELECT\n",
    "        o.order_id,\n",
    "        o.customer_id,\n",
    "        lp.payment_amount,\n",
    "        lp.paid_ts,\n",
    "        c.region\n",
    "    FROM orders o\n",
    "    INNER JOIN latest_payment_per_order lp ON o.order_id = lp.order_id\n",
    "    INNER JOIN customers c ON o.customer_id = c.customer_id\n",
    ")\n",
    "-- Step 4: Group by month and region, sum the revenue\n",
    "SELECT\n",
    "    DATE_FORMAT(paid_ts, 'yyyy-MM') as month,\n",
    "    region,\n",
    "    (SUM(payment_amount), 2) as revenue\n",
    "FROM orders_with_payments\n",
    "GROUP BY DATE_FORMAT(paid_ts, 'yyyy-MM'), region\n",
    "ORDER BY month DESC, region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fc63c82-d2ce-44d8-8f80-ecab55ec0018",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number, date_format, sum as spark_sum, current_timestamp\n",
    "\n",
    "# Step 1: Filter payments to last 90 days\n",
    "payments_90d = df_payments.filter(\n",
    "    col(\"paid_ts\") >= current_timestamp() - expr(\"INTERVAL 90 DAYS\")\n",
    ")\n",
    "\n",
    "# Step 2: Get latest payment per order using window function\n",
    "window_spec = Window.partitionBy(\"order_id\").orderBy(col(\"paid_ts\").desc())\n",
    "latest_payments = payments_90d.withColumn(\n",
    "    \"rn\", row_number().over(window_spec)\n",
    ").filter(col(\"rn\") == 1).select(\n",
    "    \"order_id\", col(\"amount\").alias(\"payment_amount\"), \"paid_ts\"\n",
    ")\n",
    "\n",
    "# Step 3: Join with orders and customers to get region\n",
    "orders_with_payments = df_orders.join(\n",
    "    latest_payments, \"order_id\", \"inner\"\n",
    ").join(\n",
    "    df_customers, \"customer_id\", \"inner\"\n",
    ")\n",
    "\n",
    "# Step 4: Group by month and region, sum the revenue\n",
    "monthly_revenue = orders_with_payments.groupBy(\n",
    "    date_format(col(\"paid_ts\"), \"yyyy-MM\").alias(\"month\"),\n",
    "    col(\"region\")\n",
    ").agg(\n",
    "    spark_sum(\"payment_amount\").alias(\"revenue\")\n",
    ").orderBy(\n",
    "    col(\"month\").desc(), col(\"region\")\n",
    ")\n",
    "\n",
    "display(monthly_revenue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecee3bcf-427b-42bc-bc5c-7521fa496f04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Verification Queries\n",
    "\n",
    "Use these queries to verify your understanding and check intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7403f116-b207-4c97-b5b5-e738ef3bc622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check: How many payments per order (to verify multiple payments exist)\n",
    "print(\"Payments per order (sample):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        order_id,\n",
    "        COUNT(*) as payment_count,\n",
    "        MIN(paid_ts) as first_payment,\n",
    "        MAX(paid_ts) as latest_payment,\n",
    "        SUM(amount) as total_paid\n",
    "    FROM payments\n",
    "    GROUP BY order_id\n",
    "    HAVING COUNT(*) > 1\n",
    "    ORDER BY payment_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dae9988d-42d5-4ff6-9204-2b3d5715af8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check: Date range of payments in last 90 days\n",
    "print(\"Payment date range (last 90 days):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        MIN(paid_ts) as earliest_payment,\n",
    "        MAX(paid_ts) as latest_payment,\n",
    "        COUNT(DISTINCT DATE_FORMAT(paid_ts, 'yyyy-MM')) as distinct_months,\n",
    "        COUNT(*) as total_payments\n",
    "    FROM payments\n",
    "    WHERE paid_ts >= current_timestamp() - INTERVAL 90 DAYS\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33b3a5e0-e35a-4285-a671-5d719a324cd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check: Sample data exploration\n",
    "print(\"Sample customers:\")\n",
    "df_customers.show()\n",
    "\n",
    "print(\"\\nSample orders:\")\n",
    "df_orders.show(10)\n",
    "\n",
    "print(\"\\nSample payments:\")\n",
    "df_payments.show(10)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7547253561083894,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_spark_sql_practice (1)",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
