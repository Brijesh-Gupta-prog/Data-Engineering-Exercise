{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cacb0a58-254d-4b49-8d35-04fcd766df82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "47YiCR5Srpiv"
   },
   "source": [
    "# Module 04 - Data Transformations & Aggregations - Exercises## InstructionsThis notebook contains exercises based on the concepts learned in Module 04.- Complete each exercise in the provided code cells- Run the data setup cells first to generate/create necessary data- Test your solutions by running the verification cells (if provided)- Refer back to the main module notebook if you need help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdb96a1b-6ab2-46fc-8fe0-d1903af44025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "txDVKZ0Vrpiw"
   },
   "source": [
    "## Data Setup\n",
    "\n",
    "Run the cells below to set up the data needed for the exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e80cc2a-8ccf-424a-930d-385e7cb57ada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZTozIe8_rpiw",
    "outputId": "3abec0a4-8ca8-4de2-b5ad-820ca149edae"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(f\"Module 4 Exercises\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set data directory\n",
    "data_dir = \"../data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "print(\"SparkSession created successfully!\")\n",
    "print(f\"Data directory: {os.path.abspath(data_dir)}\")\n",
    "\n",
    "# Generate larger dataset for aggregations (~500 MB)\n",
    "print(\"Generating large dataset for aggregations (this may take a minute)...\")\n",
    "n_records = 2_000_000  # ~500 MB of data\n",
    "\n",
    "large_data = {\n",
    "    \"employee_id\": range(1, n_records + 1),\n",
    "    \"name\": [f\"Employee_{i}\" for i in range(1, n_records + 1)],\n",
    "    \"department\": np.random.choice([\"Sales\", \"IT\", \"HR\", \"Finance\", \"Marketing\"], n_records),\n",
    "    \"salary\": np.random.randint(40000, 150000, n_records),\n",
    "    \"age\": np.random.randint(22, 65, n_records),\n",
    "    \"city\": np.random.choice(\n",
    "        [\"NYC\", \"LA\", \"Chicago\", \"Houston\", \"Phoenix\", \"Philadelphia\"], n_records\n",
    "    )\n",
    "}\n",
    "\n",
    "df_large = pd.DataFrame(large_data)\n",
    "df_large.to_csv(f\"{data_dir}/large_employees.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Created large CSV file: {data_dir}/large_employees.csv ({len(df_large)} records)\")\n",
    "\n",
    "# Also create a smaller DataFrame for quick exercises\n",
    "small_data = [\n",
    "    (\"Alice\", 25, \"Sales\", 50000, \"NYC\"),\n",
    "    (\"Bob\", 30, \"IT\", 60000, \"LA\"),\n",
    "    (\"Charlie\", 35, \"Sales\", 70000, \"Chicago\"),\n",
    "    (\"Diana\", 28, \"IT\", 55000, \"Houston\"),\n",
    "    (\"Eve\", 32, \"HR\", 65000, \"Phoenix\"),\n",
    "    (\"Frank\", 27, \"Sales\", 52000, \"NYC\"),\n",
    "    (\"Grace\", 29, \"IT\", 58000, \"LA\"),\n",
    "    (\"Henry\", 31, \"HR\", 62000, \"Chicago\")\n",
    "]\n",
    "\n",
    "small_schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Salary\", IntegerType(), True),\n",
    "    StructField(\"City\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_employees = spark.createDataFrame(small_data, small_schema)\n",
    "\n",
    "print(\"\\nSmall DataFrame for quick exercises:\")\n",
    "df_employees.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8915809c-a8be-45c5-a192-e2a88a5768f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "AHVplsRLrpix"
   },
   "source": [
    "## ExercisesComplete the following exercises based on the concepts from Module 04."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "258de2e2-3a5d-4166-a9c4-b132c0b7afca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "qVvEAJtprpix"
   },
   "source": [
    "### Exercise 1: GroupBy and AggregateGroup df_employees by Department and calculate:- Count of employees- Average salary- Maximum salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e95486d-0a64-415b-bb30-1e59490f391f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TXBliX2Hrpix",
    "outputId": "1ce4e0b5-75d3-4d37-c9ac-b74a859aef56"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg,count,max\n",
    "# Your code here\n",
    "df_1 = df_employees.groupBy(\"Department\").agg(\n",
    "    count(\"*\").alias(\"Total Count\"),\n",
    "    avg(\"Salary\").alias(\"Avarage Salary\"),\n",
    "    max(\"Salary\").alias(\"Maximum Salary\")\n",
    ")\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6857d937-109c-4e0d-aff7-52b8f8ac32dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "qk29xL05rpix"
   },
   "source": [
    "### Exercise 2: Add a ColumnAdd a new column 'Bonus' to df_employees that is 10% of the Salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ab5ec13-f350-4457-8afb-06509423422a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OOdwapNXrpix",
    "outputId": "44ee2203-c5ab-42ee-84ae-4d81df31711d"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df_2 = df_employees.withColumn(\"bonus\",col(\"Salary\")*1.1)\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d602056-1d8b-4f78-92a9-5893be37aadf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "It84GHiZrpiy"
   },
   "source": [
    "### Exercise 3: Handle Null ValuesFill null values in the 'Age' column with 0 (if any exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb38b111-1d9d-41e5-a4ab-c5ec8ecde9b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dajVNeXgrpiy",
    "outputId": "06b4757b-7fe5-4b1c-89fc-3b52d9e921c8"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df_4 = df_employees.fillna({'Age':0})\n",
    "df_4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7534a343-36ef-4156-ba7c-29bf05f8dc10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "U_RTlgTdrpiy"
   },
   "source": [
    "### Exercise 4: Large Dataset AggregationRead the large_employees.csv file and:1. Group by department2. Calculate average salary per department3. Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c74def5-f235-4e01-a7d1-26c83eec0eaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wRedPK0Drpiy",
    "outputId": "8141ddd6-d884-40a3-9059-09ce88729efd"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "data = spark.read \\\n",
    "       .format(\"csv\") \\\n",
    "       .option(\"header\",True) \\\n",
    "       .load(f\"{data_dir}/large_employees.csv\")\n",
    "\n",
    "result = data.groupBy(\"department\").agg(\n",
    "    avg(\"salary\").alias(\"Avarage Salary\")\n",
    ")\n",
    "result.show()\n",
    "\n",
    "# Note: This may take a few minutes due to dataset size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f335fc60-83e0-48e3-9dd8-c47883e255e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "DLe_0txrrpiy"
   },
   "source": [
    "## Summary\n",
    "\n",
    "Great job completing the exercises! Review your solutions and compare them with the solutions notebook if needed.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "04_data_transformations_aggregations_exercises (1)",
   "widgets": {}
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
